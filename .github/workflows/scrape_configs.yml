name: Scheduled Scrape

on:
  schedule:
    - cron: '0 */4 * * *'
  workflow_dispatch:

jobs:
  build-and-commit:
    permissions:
      contents: write
    runs-on: ubuntu-latest
    timeout-minutes: 10
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install requests

      - name: Run collector script
        run: python scraper.py
          
      - name: Upload file to ArvanCloud
        continue-on-error: true
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARVAN_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARVAN_ACCESS_KEY_SECRET }}
        run: |
          pip install awscli
          aws s3 cp all_live_configs.json s3://v2v-data/all_live_configs.json --endpoint-url https://s3.ir-thr-at1.arvanstorage.com --acl public-read

      - name: Commit and Push the file to GitHub
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add all_live_configs.json
          if git diff-index --quiet HEAD; then
            echo "No changes to commit."
          else
            git commit -m "ðŸ“Š Update raw configs"
            git push
          fi
