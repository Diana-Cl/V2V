name: Scheduled Scrape and Build

on:
  schedule:
    # Ø§Ø¬Ø±Ø§ Ù‡Ø± Û¸ Ø³Ø§Ø¹Øª ÛŒÚ©â€ŒØ¨Ø§Ø±
    - cron: "0 */8 * * *"
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-and-commit:
    permissions:
      contents: write
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }}

      - name: Create dummy download test file
        run: dd if=/dev/zero of=dl-test.bin bs=100k count=1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ø­Ø°Ù beautifulsoup4 Ú†ÙˆÙ† Ø¯Ø± Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
        run: pip install requests PyYAML PyGithub awscli

      - name: Run scraper script
        run: python scraper.py
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        timeout-minutes: 40

      - name: Upload files to ArvanCloud
        if: success() # ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø³Ú©Ø±ÛŒÙ¾Ø± Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯
        continue-on-error: true # Ø§Ú¯Ø± Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù‡
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARVAN_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARVAN_SECRET_KEY_SECRET }}
        run: |
          if [ -z "$AWS_ACCESS_KEY_ID" ]; then
            echo "::warning::ArvanCloud secrets not found, skipping upload."
            exit 0
          fi
          ENDPOINT_URL="https://s3.ir-thr-at1.arvanstorage.com"
          BUCKET_NAME="s3://v2v-data"
          
          # Ø¢Ù¾Ù„ÙˆØ¯ Ù…Ø­ØªÙˆÛŒØ§Øª Ù¾ÙˆØ´Ù‡ configs Ø¨Ù‡ Ø±ÛŒØ´Ù‡ bucket
          if [ -d "./configs" ]; then
            aws s3 sync ./configs "$BUCKET_NAME/" --endpoint-url "$ENDPOINT_URL" --acl public-read --cache-control "max-age=300"
          fi
          
          # Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø±ÛŒØ´Ù‡
          if [ -f "cache_version.txt" ]; then
            aws s3 cp cache_version.txt "$BUCKET_NAME/cache_version.txt" --endpoint-url "$ENDPOINT_URL" --acl public-read --content-type "text/plain" --cache-control "max-age=300"
          fi
          if [ -f "dl-test.bin" ]; then
            aws s3 cp dl-test.bin "$BUCKET_NAME/dl-test.bin" --endpoint-url "$ENDPOINT_URL" --acl public-read --cache-control "max-age=3600"
          fi

      - name: Commit and Push final files to GitHub
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add all_live_configs_*.json cache_version.txt clash_subscription.yaml dl-test.bin
          if git diff --staged --quiet; then
            echo "No new changes to commit."
          else
            git commit -m "ğŸ“Š Update configs [$(date -u '+%Y-%m-%d %H:%M:%S UTC')] [skip ci]"
            git push
          fi