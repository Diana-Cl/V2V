import requests
import base64
import os
import json
import re
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, parse_qs

# === CONFIGURATION ===
# Ù…Ù†Ø§Ø¨Ø¹ Ø«Ø§Ø¨Øª (18 Ù…Ù†Ø¨Ø¹ Ù…Ø·Ù…Ø¦Ù†)
BASE_SUBSCRIPTION_SOURCES = [
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub1.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub2.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub3.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub4.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub5.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub6.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub7.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub8.txt",
    "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge.txt",
    "https://raw.githubusercontent.com/NiREvil/vless/refs/heads/main/sub/SSTime",
    "https://raw.githubusercontent.com/itsyebekhe/PSG/main/lite/subscriptions/xray/normal/mix",
    "https://raw.githubusercontent.com/arshiacomplus/v2rayExtractor/refs/heads/main/mix/sub.html",
    "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub1.txt",
    "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub2.txt",
    "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub3.txt",
    "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub4.txt",
    "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub5.txt",
    "https://raw.githubusercontent.com/youfoundamin/V2rayCollector/main/mixed_iran.txt",
    "https://raw.githubusercontent.com/hamedcode/port-based-v2ray-configs/main/sub/port_8443.txt",
    "https://raw.githubusercontent.com/hamedcode/port-based-v2ray-configs/main/detailed/vless/2087.txt",
    "https://raw.githubusercontent.com/lagzian/SS-Collector/refs/heads/main/mix.txt",
    "https://raw.githubusercontent.com/MahsaNetConfigTopic/config/main/xray_final.txt"
]

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ù„ÛŒ
OUTPUT_JSON_FILE = 'all_live_configs.json'
VALID_PREFIXES = ('vless://', 'vmess://', 'trojan://', 'ss://', 'hysteria2://', 'hy2://', 'tuic://')
GITHUB_PAT = os.environ.get('GH_PAT')
HEADERS = {'User-Agent': 'V2V-Scraper/Complete-v3.0'}
if GITHUB_PAT:
    HEADERS['Authorization'] = f'token {GITHUB_PAT}'

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªØ³Øª Ø³Ø±Ø¹Øª
TARGET_CONFIGS_PER_CORE = 500  # 500 Ø¨Ø±Ø§ÛŒ Ù‡Ø± core
MAX_PING_THRESHOLD = 1000      # Ø­Ø¯Ø§Ú©Ø«Ø± 1000ms
API_ENDPOINT = 'https://v2-v.vercel.app/api/proxy'  # API ÙˆØ±Ø³Ù„ Ø´Ù…Ø§
BATCH_SIZE = 15                # ØªØ¹Ø¯Ø§Ø¯ ØªØ³Øª Ù‡Ù…Ø²Ù…Ø§Ù†
MAX_WORKERS = 25               # ØªØ¹Ø¯Ø§Ø¯ thread
REQUEST_TIMEOUT = 8            # timeout Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª API
GITHUB_SEARCH_LIMIT = 30       # Ø­Ø¯Ø§Ú©Ø«Ø± repo Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ

# Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ GitHub
GITHUB_SEARCH_QUERIES = [
    'v2ray subscription',
    'vmess config',
    'vless subscription',
    'trojan config',
    'xray config',
    'clash subscription',
    'v2ray configs',
    'proxy subscription'
]

# === GITHUB SEARCH FUNCTIONS ===
def search_github_repositories(query: str, max_results: int = 10) -> list:
    """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± GitHub Ø¨Ø±Ø§ÛŒ repository Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·"""
    if not GITHUB_PAT:
        return []
    
    try:
        url = 'https://api.github.com/search/repositories'
        params = {
            'q': f'{query} language:text sort:updated',
            'sort': 'updated',
            'order': 'desc',
            'per_page': max_results
        }
        
        response = requests.get(url, params=params, headers=HEADERS, timeout=10)
        if response.status_code != 200:
            return []
        
        data = response.json()
        repos = []
        
        for item in data.get('items', []):
            # ÙÛŒÙ„ØªØ± repository Ù‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨
            if (item.get('size', 0) < 50000 and  # Ù†Ù‡ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯
                not item.get('fork', False) and   # Ø§ØµÙ„ÛŒ Ø¨Ø§Ø´Ù‡ Ù†Ù‡ fork
                item.get('updated_at')):          # Ø§Ø®ÛŒØ±Ø§Ù‹ Ø¨Ø±ÙˆØ² Ø´Ø¯Ù‡
                
                repos.append({
                    'owner': item['owner']['login'],
                    'name': item['name'],
                    'full_name': item['full_name']
                })
        
        return repos
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ GitHub: {e}")
        return []

def get_repository_files(owner: str, repo: str) -> list:
    """Ø¯Ø±ÛŒØ§ÙØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø² ÛŒÚ© repository"""
    if not GITHUB_PAT:
        return []
    
    try:
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± root Ùˆ Ø²ÛŒØ±Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„
        paths_to_check = ['', 'sub', 'subs', 'subscription', 'config', 'configs']
        file_urls = []
        
        for path in paths_to_check:
            url = f'https://api.github.com/repos/{owner}/{repo}/contents/{path}'
            try:
                response = requests.get(url, headers=HEADERS, timeout=8)
                if response.status_code != 200:
                    continue
                
                contents = response.json()
                if isinstance(contents, list):
                    for item in contents:
                        if (item.get('type') == 'file' and
                            item.get('name', '').lower().endswith(('.txt', '.yaml', '.yml', '.sub'))):
                            file_urls.append(item['download_url'])
                        
                        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø§Ø² Ù‡Ø± repo
                        if len(file_urls) >= 5:
                            break
            except:
                continue
            
            if len(file_urls) >= 5:
                break
        
        return file_urls[:5]  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 ÙØ§ÛŒÙ„ Ø§Ø² Ù‡Ø± repo
    except Exception:
        return []

def discover_dynamic_sources() -> list:
    """Ú©Ø´Ù Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§ Ø§Ø² GitHub"""
    print("ğŸ” Ú©Ø´Ù Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§ Ø§Ø² GitHub...")
    dynamic_sources = []
    
    if not GITHUB_PAT:
        print("âš ï¸ GitHub PAT ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§ ØµØ±Ùâ€ŒÙ†Ø¸Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        return []
    
    try:
        for query in GITHUB_SEARCH_QUERIES[:3]:  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¬Ø³ØªØ¬Ùˆ
            repos = search_github_repositories(query, max_results=5)
            
            for repo in repos:
                file_urls = get_repository_files(repo['owner'], repo['name'])
                dynamic_sources.extend(file_urls)
                
                # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ú©Ù„ Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§
                if len(dynamic_sources) >= GITHUB_SEARCH_LIMIT:
                    break
            
            if len(dynamic_sources) >= GITHUB_SEARCH_LIMIT:
                break
            
            time.sleep(1)  # Rate limiting
    
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ú©Ø´Ù Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§: {e}")
    
    # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
    unique_sources = list(set(dynamic_sources))
    print(f"âœ… {len(unique_sources)} Ù…Ù†Ø¨Ø¹ Ù¾ÙˆÛŒØ§ Ú©Ø´Ù Ø´Ø¯")
    return unique_sources

# === HELPER FUNCTIONS ===
def get_content_from_url(url: str) -> str | None:
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø­ØªÙˆØ§ Ø§Ø² URL"""
    try:
        response = requests.get(url, timeout=15, headers=HEADERS)
        response.raise_for_status()
        return response.text
    except Exception:
        return None

def decode_content(content: str) -> list[str]:
    """Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ Ù…Ø­ØªÙˆØ§ÛŒ base64 ÛŒØ§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù† Ø®Ø·ÙˆØ· Ù…Ø³ØªÙ‚ÛŒÙ…"""
    try:
        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ decode base64
        decoded = base64.b64decode(content).decode('utf-8').strip()
        return decoded.splitlines()
    except Exception:
        # Ø§Ú¯Ø± base64 Ù†Ø¨ÙˆØ¯ØŒ Ø®Ø·ÙˆØ· Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
        return content.strip().splitlines()

def fetch_and_parse_url(url: str) -> set[str]:
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø² ÛŒÚ© URL"""
    content = get_content_from_url(url)
    if not content:
        return set()
    
    configs = set()
    lines = decode_content(content)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ø®Ø·ÙˆØ· Ù…Ø³ØªÙ‚ÛŒÙ…
    for line in lines:
        line = line.strip()
        if line.startswith(VALID_PREFIXES):
            configs.add(line)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² HTML Ø¨Ø§ regex
    pattern = r'(' + '|'.join([p.replace('://', r'://[^\s\'"<>]+') for p in VALID_PREFIXES]) + ')'
    found_configs = re.findall(pattern, content)
    for config in found_configs:
        configs.add(config.strip())
    
    return configs

def parse_server_details(config_url: str) -> dict | None:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ host Ùˆ port Ø§Ø² Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø±Ø§ÛŒ ØªØ³Øª ping"""
    try:
        # Handle SS protocol
        if config_url.startswith('ss://'):
            at_index = config_url.rfind('@')
            if at_index == -1:
                return None
            
            host_part = config_url[at_index + 1:]
            if '#' in host_part:
                host_part = host_part[:host_part.rfind('#')]
            
            colon_index = host_part.rfind(':')
            if colon_index == -1:
                return None
            
            host = host_part[:colon_index]
            try:
                port = int(host_part[colon_index + 1:])
                return {'host': host, 'port': port}
            except ValueError:
                return None
        
        # Handle VMESS protocol
        if config_url.startswith('vmess://'):
            try:
                parsed = urlparse(config_url)
                b64_data = parsed.hostname
                
                # Add padding if needed
                missing_padding = len(b64_data) % 4
                if missing_padding:
                    b64_data += '=' * (4 - missing_padding)
                
                decoded = json.loads(base64.b64decode(b64_data).decode('utf-8'))
                host = decoded.get('add')
                port = int(decoded.get('port', 0))
                
                if host and port:
                    return {'host': host, 'port': port}
                return None
            except Exception:
                return None
        
        # Handle other protocols (vless, trojan, etc.)
        parsed = urlparse(config_url)
        if not parsed.hostname:
            return None
        
        port = parsed.port
        if not port:
            # Default ports based on protocol
            default_ports = {
                'vless': 443, 'trojan': 443, 'hysteria2': 443,
                'hy2': 443, 'tuic': 443
            }
            port = default_ports.get(parsed.scheme, 443)
        
        return {'host': parsed.hostname, 'port': port}
    
    except Exception:
        return None

def test_config_via_vercel_api(config_url: str) -> dict:
    """ØªØ³Øª Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø² Ø·Ø±ÛŒÙ‚ API ÙˆØ±Ø³Ù„"""
    server_details = parse_server_details(config_url)
    
    if not server_details:
        return {
            'config_str': config_url,
            'ping': 9999,
            'status': 'parse_error'
        }
    
    try:
        # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ API ÙˆØ±Ø³Ù„
        response = requests.post(
            API_ENDPOINT,
            json={
                'host': server_details['host'],
                'port': server_details['port']
            },
            headers={'Content-Type': 'application/json'},
            timeout=REQUEST_TIMEOUT
        )
        
        if response.status_code == 200:
            data = response.json()
            ping = data.get('ping', 9999)
            
            return {
                'config_str': config_url,
                'ping': ping,
                'status': 'success' if ping < 9999 else 'timeout'
            }
        else:
            return {
                'config_str': config_url,
                'ping': 9999,
                'status': f'api_error_{response.status_code}'
            }
    
    except requests.exceptions.Timeout:
        return {
            'config_str': config_url,
            'ping': 9999,
            'status': 'timeout'
        }
    except Exception as e:
        return {
            'config_str': config_url,
            'ping': 9999,
            'status': 'network_error'
        }

def validate_and_categorize_config(config_url: str) -> dict | None:
    """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ syntax Ùˆ ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ core"""
    try:
        parsed_url = urlparse(config_url)
        protocol = parsed_url.scheme.lower()
        
        # Basic validation
        if not parsed_url.hostname and protocol != 'vmess':
            return None
        
        # Determine core type
        core = 'xray'
        if protocol in ['hysteria2', 'hy2', 'tuic']:
            core = 'singbox'
        elif protocol == 'vless':
            query_params = parse_qs(parsed_url.query)
            if query_params.get('security', [''])[0] == 'reality':
                core = 'singbox'
                # Validate REALITY public key
                pbk = query_params.get('pbk', [None])[0]
                if not pbk or not re.match(r'^[A-Za-z0-9-_]{43}$', pbk):
                    return None  # Invalid REALITY config
        
        return {
            'core': core,
            'config_str': config_url
        }
    except Exception:
        return None

def process_configs_batch(configs_batch: list) -> list:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© batch Ø§Ø² Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§"""
    results = []
    
    with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:
        # Ø§Ø±Ø³Ø§Ù„ Ù‡Ù…Ø²Ù…Ø§Ù† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
        future_to_config = {
            executor.submit(test_config_via_vercel_api, cfg): cfg 
            for cfg in configs_batch
        }
        
        for future in as_completed(future_to_config):
            try:
                result = future.result()
                # ÙÙ‚Ø· Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± threshold Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
                if result['ping'] <= MAX_PING_THRESHOLD:
                    results.append(result)
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª Ú©Ø§Ù†ÙÛŒÚ¯: {e}")
    
    return results

# === MAIN EXECUTION ===
def main():
    print("ğŸš€ V2V Enhanced Scraper - Ù…Ù†Ø§Ø¨Ø¹ Ø«Ø§Ø¨Øª + GitHub Search + ØªØ³Øª ÙˆØ±Ø³Ù„")
    print(f"ğŸ¯ Ù‡Ø¯Ù: {TARGET_CONFIGS_PER_CORE} Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± core")
    print(f"âš¡ Ø­Ø¯Ø§Ú©Ø«Ø± ping: {MAX_PING_THRESHOLD}ms")
    
    # 1. ØªØ±Ú©ÛŒØ¨ Ù…Ù†Ø§Ø¨Ø¹ Ø«Ø§Ø¨Øª + Ù¾ÙˆÛŒØ§
    all_sources = BASE_SUBSCRIPTION_SOURCES.copy()
    dynamic_sources = discover_dynamic_sources()
    all_sources.extend(dynamic_sources)
    
    print(f"ğŸ“¡ Ù…Ø¬Ù…ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹: {len(BASE_SUBSCRIPTION_SOURCES)} Ø«Ø§Ø¨Øª + {len(dynamic_sources)} Ù¾ÙˆÛŒØ§ = {len(all_sources)}")
    
    # 2. Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§
    print("ğŸšš Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§...")
    all_configs_raw = set()
    
    with ThreadPoolExecutor(max_workers=20) as executor:
        results = executor.map(fetch_and_parse_url, all_sources)
        for config_set in results:
            all_configs_raw.update(config_set)
    
    print(f"ğŸ“¦ Ù…Ø¬Ù…ÙˆØ¹ {len(all_configs_raw)} Ú©Ø§Ù†ÙÛŒÚ¯ Ø®Ø§Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
    
    if len(all_configs_raw) == 0:
        print("âŒ Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        return
    
    # 3. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ syntax Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
    print("ğŸ”¬ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ syntax Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ...")
    categorized_configs = {'xray': [], 'singbox': []}
    
    with ThreadPoolExecutor(max_workers=50) as executor:
        future_to_config = {
            executor.submit(validate_and_categorize_config, cfg): cfg 
            for cfg in all_configs_raw
        }
        
        for future in as_completed(future_to_config):
            result = future.result()
            if result:
                core = result.get('core')
                if core in categorized_configs:
                    categorized_configs[core].append(result['config_str'])
    
    print(f"âœ… Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±: Xray={len(categorized_configs['xray'])}, Singbox={len(categorized_configs['singbox'])}")
    
    # 4. ØªØ³Øª Ø³Ø±Ø¹Øª Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
    final_configs = {'xray': [], 'singbox': []}
    
    for core_name, configs in categorized_configs.items():
        if not configs:
            print(f"âš ï¸ Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ {core_name.upper()} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            continue
        
        print(f"\nğŸƒ ØªØ³Øª Ø³Ø±Ø¹Øª {len(configs)} Ú©Ø§Ù†ÙÛŒÚ¯ {core_name.upper()}...")
        tested_configs = []
        
        # ØªØ³Øª Ø¯Ø± batch Ù‡Ø§ÛŒ Ú©ÙˆÚ†Ú©
        total_batches = (len(configs) + BATCH_SIZE - 1) // BATCH_SIZE
        
        for i in range(0, len(configs), BATCH_SIZE):
            batch = configs[i:i+BATCH_SIZE]
            batch_num = (i // BATCH_SIZE) + 1
            
            print(f"   ğŸ“¡ Batch {batch_num}/{total_batches}: {len(batch)} Ú©Ø§Ù†ÙÛŒÚ¯")
            
            batch_results = process_configs_batch(batch)
            tested_configs.extend(batch_results)
            
            # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
            fast_count = len([c for c in tested_configs if c['ping'] <= MAX_PING_THRESHOLD])
            print(f"   âš¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯ Ø³Ø±ÛŒØ¹ ØªØ§ Ú©Ù†ÙˆÙ†: {fast_count}")
            
            # Ø§Ú¯Ø± Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ù†ÛŒØ§Ø² ÛŒØ§ÙØª Ø´Ø¯ØŒ ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…
            if len(tested_configs) >= TARGET_CONFIGS_PER_CORE * 3:
                print(f"   ğŸ¯ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§ÙÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ø³Ø±ÛŒØ¹ ÛŒØ§ÙØª Ø´Ø¯ØŒ ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…")
                break
            
            # Ø§Ø³ØªØ±Ø§Ø­Øª Ú©ÙˆØªØ§Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overload
            time.sleep(0.3)
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ping Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
        tested_configs.sort(key=lambda x: x['ping'])
        best_configs = tested_configs[:TARGET_CONFIGS_PER_CORE]
        
        final_configs[core_name] = [
            {
                'config_str': cfg['config_str'],
                'ping': cfg['ping']
            }
            for cfg in best_configs
        ]
        
        print(f"ğŸ¯ {core_name.upper()}: {len(final_configs[core_name])} Ú©Ø§Ù†ÙÛŒÚ¯ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯")
        
        if final_configs[core_name]:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
            pings = [c['ping'] for c in final_configs[core_name]]
            avg_ping = sum(pings) / len(pings)
            min_ping = min(pings)
            max_ping = max(pings)
            
            print(f"ğŸ“Š Ø¢Ù…Ø§Ø± ping: Ø­Ø¯Ø§Ù‚Ù„={min_ping}ms, Ø­Ø¯Ø§Ú©Ø«Ø±={max_ping}ms, Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={avg_ping:.1f}ms")
    
    # 5. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ
    total_configs = len(final_configs['xray']) + len(final_configs['singbox'])
    print(f"\nğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ {total_configs} Ú©Ø§Ù†ÙÛŒÚ¯ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± {OUTPUT_JSON_FILE}...")
    
    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_configs, f, ensure_ascii=False, indent=2)
    
    # 6. Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
    print("\nğŸ‰ ÙØ±Ø¢ÛŒÙ†Ø¯ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
    print("ğŸ“ˆ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:")
    print(f"   ğŸ”¸ Xray: {len(final_configs['xray'])} Ú©Ø§Ù†ÙÛŒÚ¯")
    print(f"   ğŸ”¸ Singbox: {len(final_configs['singbox'])} Ú©Ø§Ù†ÙÛŒÚ¯")
    print(f"   ğŸ”¸ Ù…Ø¬Ù…ÙˆØ¹: {total_configs} Ú©Ø§Ù†ÙÛŒÚ¯")
    print(f"ğŸ’¾ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ: {OUTPUT_JSON_FILE}")
    print(f"ğŸŒ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡: {len(all_sources)} Ù…Ù†Ø¨Ø¹")
    
    if total_configs > 0:
        print("âœ… ÙØ§ÛŒÙ„ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø³Ø§ÛŒØª!")
    else:
        print("âš ï¸ Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ø³Ø§Ù„Ù…ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")

if __name__ == "__main__":
    main()