# === scraper.py (Final Merged Version) ===
import requests
import base64
import os
import json
import socket
import time
import yaml
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, unquote, parse_qs

# === CONFIGURATION ===
BASE_SOURCES = [
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub1.txt", "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub2.txt", "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub3.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub4.txt", "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub5.txt", "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub6.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub7.txt", "https://raw.githubusercontent.com/barry-far/V2ray-Config/main/Sub8.txt", "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge.txt",
    "https://raw.githubusercontent.com/NiREvil/vless/refs/heads/main/sub/SSTime", "https://raw.githubusercontent.com/itsyebekhe/PSG/main/lite/subscriptions/xray/normal/mix", "https://raw.githubusercontent.com/arshiacomplus/v2rayExtractor/refs/heads/main/mix/sub.html",
    "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub1.txt", "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub2.txt", "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub3.txt",
    "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub4.txt", "https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub5.txt", "https://raw.githubusercontent.com/youfoundamin/V2rayCollector/main/mixed_iran.txt",
    "https://raw.githubusercontent.com/hamedcode/port-based-v2ray-configs/main/sub/port_8443.txt", "https://raw.githubusercontent.com/hamedcode/port-based-v2ray-configs/main/detailed/vless/2087.txt", "https://raw.githubusercontent.com/lagzian/SS-Collector/refs/heads/main/mix.txt",
    "https://raw.githubusercontent.com/MahsaNetConfigTopic/config/main/xray_final.txt"
]

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ù„ÛŒ
OUTPUT_JSON_FILE = 'all_live_configs.json'
VALID_PREFIXES = ('vless://', 'vmess://', 'trojan://', 'ss://', 'hysteria2://', 'hy2://', 'tuic://', 'wg://') # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† ss Ùˆ wg
GITHUB_PAT = os.environ.get('GH_PAT')
HEADERS = {'User-Agent': 'V2V-Scraper/Complete-v3.0'}
if GITHUB_PAT:
    HEADERS['Authorization'] = f'token {GITHUB_PAT}'

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªØ³Øª Ø³Ø±Ø¹Øª
TARGET_CONFIGS_PER_CORE = 500  # 500 Ø¨Ø±Ø§ÛŒ Ù‡Ø± core
MAX_PING_THRESHOLD = 1000      # Ø­Ø¯Ø§Ú©Ø«Ø± 1000ms
API_ENDPOINT = 'https://v2-v.vercel.app/api/proxy'
BATCH_SIZE = 15                # ØªØ¹Ø¯Ø§Ø¯ ØªØ³Øª Ù‡Ù…Ø²Ù…Ø§Ù†
MAX_WORKERS = 25               # ØªØ¹Ø¯Ø§Ø¯ thread
REQUEST_TIMEOUT = 8            # timeout Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª API
GITHUB_SEARCH_LIMIT = 30       # Ø­Ø¯Ø§Ú©Ø«Ø± repo Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ

# Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ GitHub
GITHUB_SEARCH_QUERIES = [
    'v2ray subscription',
    'vmess config',
    'vless subscription',
    'trojan config',
    'xray config',
    'clash subscription',
    'v2ray configs',
    'proxy subscription'
]

# === GITHUB SEARCH FUNCTIONS ===
def search_github_repositories(query: str, max_results: int = 10) -> list:
    """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± GitHub Ø¨Ø±Ø§ÛŒ repository Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·"""
    if not GITHUB_PAT:
        return []
    
    try:
        url = 'https://api.github.com/search/repositories'
        params = {
            'q': f'{query} language:text sort:updated',
            'sort': 'updated',
            'order': 'desc',
            'per_page': max_results
        }
        
        response = requests.get(url, params=params, headers=HEADERS, timeout=10)
        if response.status_code != 200:
            return []
        
        data = response.json()
        repos = []
        
        for item in data.get('items', []):
            if (item.get('size', 0) < 50000 and
                not item.get('fork', False) and
                item.get('updated_at')):
                
                repos.append({
                    'owner': item['owner']['login'],
                    'name': item['name'],
                    'full_name': item['full_name']
                })
        
        return repos
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ GitHub: {e}")
        return []

def get_repository_files(owner: str, repo: str) -> list:
    """Ø¯Ø±ÛŒØ§ÙØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø² ÛŒÚ© repository"""
    if not GITHUB_PAT:
        return []
    
    try:
        paths_to_check = ['', 'sub', 'subs', 'subscription', 'config', 'configs']
        file_urls = []
        
        for path in paths_to_check:
            url = f'https://api.github.com/repos/{owner}/{repo}/contents/{path}'
            try:
                response = requests.get(url, headers=HEADERS, timeout=8)
                if response.status_code != 200:
                    continue
                
                contents = response.json()
                if isinstance(contents, list):
                    for item in contents:
                        if (item.get('type') == 'file' and
                            item.get('name', '').lower().endswith(('.txt', '.yaml', '.yml', '.sub'))):
                            file_urls.append(item['download_url'])
                        
                        if len(file_urls) >= 5:
                            break
            except:
                continue
            
            if len(file_urls) >= 5:
                break
        
        return file_urls[:5]
    except Exception:
        return []

def discover_dynamic_sources() -> list:
    """Ú©Ø´Ù Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§ Ø§Ø² GitHub"""
    print("ğŸ” Ú©Ø´Ù Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§ Ø§Ø² GitHub...")
    dynamic_sources = []
    
    if not GITHUB_PAT:
        print("âš ï¸ GitHub PAT ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§ ØµØ±Ùâ€ŒÙ†Ø¸Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        return []
    
    try:
        for query in GITHUB_SEARCH_QUERIES[:3]:
            repos = search_github_repositories(query, max_results=5)
            
            for repo in repos:
                file_urls = get_repository_files(repo['owner'], repo['name'])
                dynamic_sources.extend(file_urls)
                
                if len(dynamic_sources) >= GITHUB_SEARCH_LIMIT:
                    break
            
            if len(dynamic_sources) >= GITHUB_SEARCH_LIMIT:
                break
            
            time.sleep(1)
    
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ú©Ø´Ù Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÙˆÛŒØ§: {e}")
    
    unique_sources = list(set(dynamic_sources))
    print(f"âœ… {len(unique_sources)} Ù…Ù†Ø¨Ø¹ Ù¾ÙˆÛŒØ§ Ú©Ø´Ù Ø´Ø¯")
    return unique_sources

# === HELPER FUNCTIONS ===
def get_content_from_url(url: str) -> str | None:
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø­ØªÙˆØ§ Ø§Ø² URL Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
    try:
        response = requests.get(url, timeout=15, headers=HEADERS)
        response.raise_for_status()
        return response.text
    except requests.exceptions.HTTPError as errh:
        print(f"âŒ HTTP Error: {errh}")
    except requests.exceptions.ConnectionError as errc:
        print(f"âŒ Connection Error: {errc}")
    except requests.exceptions.Timeout as errt:
        print(f"âŒ Timeout Error: {errt}")
    except requests.exceptions.RequestException as err:
        print(f"âŒ Other Request Error: {err}")
    return None

def decode_content(content: str) -> list[str]:
    """Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ Ù…Ø­ØªÙˆØ§ÛŒ base64 ÛŒØ§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù† Ø®Ø·ÙˆØ· Ù…Ø³ØªÙ‚ÛŒÙ…"""
    try:
        decoded = base64.b64decode(content).decode('utf-8').strip()
        return decoded.splitlines()
    except Exception:
        return content.strip().splitlines()

def fetch_and_parse_url(url: str) -> set[str]:
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø² ÛŒÚ© URL"""
    content = get_content_from_url(url)
    if not content:
        return set()
    
    configs = set()
    lines = decode_content(content)
    
    for line in lines:
        line = line.strip()
        if line.startswith(VALID_PREFIXES):
            configs.add(line)
    
    pattern = r'(' + '|'.join([p.replace('://', r'://[^\s\'"<]+') for p in VALID_PREFIXES]) + ')'
    found_configs = re.findall(pattern, content)
    for config in found_configs:
        configs.add(config.strip())
    
    return configs

def parse_server_details(config_url: str) -> dict | None:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ host Ùˆ port Ø§Ø² Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø±Ø§ÛŒ ØªØ³Øª ping (Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯)"""
    try:
        parsed_url = urlparse(config_url)
        protocol = parsed_url.scheme.lower()
        
        # Handle SS protocol
        if protocol == 'ss':
            at_index = config_url.rfind('@')
            if at_index == -1:
                return None
            host_part = config_url[at_index + 1:]
            if '#' in host_part:
                host_part = host_part[:host_part.rfind('#')]
            colon_index = host_part.rfind(':')
            if colon_index == -1:
                return None
            host = host_part[:colon_index]
            try:
                port = int(host_part[colon_index + 1:])
                return {'host': host, 'port': port}
            except ValueError:
                return None
        
        # Handle VMESS protocol
        if protocol == 'vmess':
            try:
                parsed = urlparse(config_url)
                b64_data = parsed.hostname
                missing_padding = len(b64_data) % 4
                if missing_padding:
                    b64_data += '=' * (4 - missing_padding)
                decoded = json.loads(base64.b64decode(b64_data).decode('utf-8'))
                host = decoded.get('add')
                port = int(decoded.get('port', 0))
                if host and port:
                    return {'host': host, 'port': port}
                return None
            except Exception:
                return None
        
        # Handle other protocols (vless, trojan, wg, etc.)
        if not parsed_url.hostname:
            return None
        
        port = parsed_url.port
        if not port:
            default_ports = {
                'vless': 443, 'trojan': 443, 'hysteria2': 443,
                'hy2': 443, 'tuic': 443, 'wg': 443
            }
            port = default_ports.get(protocol, 443)
        
        return {'host': parsed_url.hostname, 'port': port}
    
    except Exception:
        return None

def test_config_via_vercel_api(config_url: str) -> dict:
    """ØªØ³Øª Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø² Ø·Ø±ÛŒÙ‚ API ÙˆØ±Ø³Ù„"""
    server_details = parse_server_details(config_url)
    
    if not server_details:
        return {
            'config_str': config_url,
            'ping': 9999,
            'status': 'parse_error'
        }
    
    try:
        response = requests.post(
            API_ENDPOINT,
            json={
                'host': server_details['host'],
                'port': server_details['port']
            },
            headers={'Content-Type': 'application/json'},
            timeout=REQUEST_TIMEOUT
        )
        
        if response.status_code == 200:
            data = response.json()
            ping = data.get('ping', 9999)
            
            return {
                'config_str': config_url,
                'ping': ping,
                'status': 'success' if ping < 9999 else 'timeout'
            }
        else:
            return {
                'config_str': config_url,
                'ping': 9999,
                'status': f'api_error_{response.status_code}'
            }
    
    except requests.exceptions.Timeout:
        return {
            'config_str': config_url,
            'ping': 9999,
            'status': 'timeout'
        }
    except Exception as e:
        return {
            'config_str': config_url,
            'ping': 9999,
            'status': 'network_error'
        }

def validate_and_categorize_config(config_url: str) -> dict | None:
    """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ syntax Ùˆ ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ core"""
    try:
        parsed_url = urlparse(config_url)
        protocol = parsed_url.scheme.lower()
        
        if not parsed_url.hostname and protocol not in ['vmess', 'ss', 'wg']:
            return None
        
        core = 'xray'
        if protocol in ['hysteria2', 'hy2', 'tuic', 'wg']:
            core = 'singbox'
        elif protocol == 'vless':
            query_params = parse_qs(parsed_url.query)
            if query_params.get('security', [''])[0] == 'reality':
                core = 'singbox'
                pbk = query_params.get('pbk', [None])[0]
                if not pbk or not re.match(r'^[A-Za-z0-9-_]{43}$', pbk):
                    return None
        
        return {
            'core': core,
            'config_str': config_url
        }
    except Exception:
        return None

def process_configs_batch(configs_batch: list) -> list:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© batch Ø§Ø² Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§"""
    results = []
    
    with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:
        future_to_config = {
            executor.submit(test_config_via_vercel_api, cfg): cfg 
            for cfg in configs_batch
        }
        
        for future in as_completed(future_to_config):
            try:
                result = future.result()
                if result['ping'] <= MAX_PING_THRESHOLD:
                    results.append(result)
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª Ú©Ø§Ù†ÙÛŒÚ¯: {e}")
    
    return results

def main():
    print("ğŸš€ V2V Enhanced Scraper - Ù…Ù†Ø§Ø¨Ø¹ Ø«Ø§Ø¨Øª + GitHub Search + ØªØ³Øª ÙˆØ±Ø³Ù„")
    print(f"ğŸ¯ Ù‡Ø¯Ù: {TARGET_CONFIGS_PER_CORE} Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± core")
    print(f"âš¡ Ø­Ø¯Ø§Ú©Ø«Ø± ping: {MAX_PING_THRESHOLD}ms")
    
    all_sources = BASE_SUBSCRIPTION_SOURCES.copy()
    dynamic_sources = discover_dynamic_sources()
    all_sources.extend(dynamic_sources)
    
    print(f"ğŸ“¡ Ù…Ø¬Ù…ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹: {len(BASE_SUBSCRIPTION_SOURCES)} Ø«Ø§Ø¨Øª + {len(dynamic_sources)} Ù¾ÙˆÛŒØ§ = {len(all_sources)}")
    
    print("ğŸšš Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§...")
    all_configs_raw = set()
    
    with ThreadPoolExecutor(max_workers=20) as executor:
        results = executor.map(fetch_and_parse_url, all_sources)
        for config_set in results:
            all_configs_raw.update(config_set)
    
    print(f"ğŸ“¦ Ù…Ø¬Ù…ÙˆØ¹ {len(all_configs_raw)} Ú©Ø§Ù†ÙÛŒÚ¯ Ø®Ø§Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
    
    if len(all_configs_raw) == 0:
        print("âŒ Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:
             json.dump({"xray": [], "singbox": []}, f, ensure_ascii=False, indent=2)
        return
    
    print("ğŸ”¬ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ syntax Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ...")
    categorized_configs = {'xray': [], 'singbox': []}
    
    with ThreadPoolExecutor(max_workers=50) as executor:
        future_to_config = {
            executor.submit(validate_and_categorize_config, cfg): cfg 
            for cfg in all_configs_raw
        }
        
        for future in as_completed(future_to_config):
            result = future.result()
            if result:
                core = result.get('core')
                if core in categorized_configs:
                    categorized_configs[core].append(result['config_str'])
    
    print(f"âœ… Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±: Xray={len(categorized_configs['xray'])}, Singbox={len(categorized_configs['singbox'])}")
    
    final_configs = {'xray': [], 'singbox': []}
    
    for core_name, configs in categorized_configs.items():
        if not configs:
            print(f"âš ï¸ Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ {core_name.upper()} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            continue
        
        print(f"\nğŸƒ ØªØ³Øª Ø³Ø±Ø¹Øª {len(configs)} Ú©Ø§Ù†ÙÛŒÚ¯ {core_name.upper()}...")
        tested_configs = []
        
        total_batches = (len(configs) + BATCH_SIZE - 1) // BATCH_SIZE
        
        for i in range(0, len(configs), BATCH_SIZE):
            batch = configs[i:i+BATCH_SIZE]
            batch_num = (i // BATCH_SIZE) + 1
            
            print(f"   ğŸ“¡ Batch {batch_num}/{total_batches}: {len(batch)} Ú©Ø§Ù†ÙÛŒÚ¯")
            
            batch_results = process_configs_batch(batch)
            tested_configs.extend(batch_results)
            
            fast_count = len([c for c in tested_configs if c['ping'] <= MAX_PING_THRESHOLD])
            print(f"   âš¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯ Ø³Ø±ÛŒØ¹ ØªØ§ Ú©Ù†ÙˆÙ†: {fast_count}")
            
            if len(tested_configs) >= TARGET_CONFIGS_PER_CORE * 3:
                print(f"   ğŸ¯ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§ÙÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ø³Ø±ÛŒØ¹ ÛŒØ§ÙØª Ø´Ø¯ØŒ ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…")
                break
            
            time.sleep(0.3)
        
        tested_configs.sort(key=lambda x: x['ping'])
        best_configs = tested_configs[:TARGET_CONFIGS_PER_CORE]
        
        final_configs[core_name] = [
            {
                'config_str': cfg['config_str'],
                'ping': cfg['ping']
            }
            for cfg in best_configs
        ]
        
        print(f"ğŸ¯ {core_name.upper()}: {len(final_configs[core_name])} Ú©Ø§Ù†ÙÛŒÚ¯ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯")
        
        if final_configs[core_name]:
            pings = [c['ping'] for c in final_configs[core_name]]
            avg_ping = sum(pings) / len(pings)
            min_ping = min(pings)
            max_ping = max(pings)
            
            print(f"ğŸ“Š Ø¢Ù…Ø§Ø± ping: Ø­Ø¯Ø§Ù‚Ù„={min_ping}ms, Ø­Ø¯Ø§Ú©Ø«Ø±={max_ping}ms, Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={avg_ping:.1f}ms")
    
    total_configs = len(final_configs['xray']) + len(final_configs['singbox'])
    print(f"\nğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ {total_configs} Ú©Ø§Ù†ÙÛŒÚ¯ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± {OUTPUT_JSON_FILE}...")
    
    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_configs, f, ensure_ascii=False, indent=2)
    
    print("\nğŸ‰ ÙØ±Ø¢ÛŒÙ†Ø¯ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
    print("ğŸ“ˆ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:")
    print(f"   ğŸ”¸ Xray: {len(final_configs['xray'])} Ú©Ø§Ù†ÙÛŒÚ¯")
    print(f"   ğŸ”¸ Singbox: {len(final_configs['singbox'])} Ú©Ø§Ù†ÙÛŒÚ¯")
    print(f"   ğŸ”¸ Ù…Ø¬Ù…ÙˆØ¹: {total_configs} Ú©Ø§Ù†ÙÛŒÚ¯")
    print(f"ğŸ’¾ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ: {OUTPUT_JSON_FILE}")
    print(f"ğŸŒ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡: {len(all_sources)} Ù…Ù†Ø¨Ø¹")
    
    if total_configs > 0:
        print("âœ… ÙØ§ÛŒÙ„ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø³Ø§ÛŒØª!")
    else:
        print("âš ï¸ Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ø³Ø§Ù„Ù…ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")

if __name__ == "__main__":
    main()
